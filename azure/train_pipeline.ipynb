{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import azureml.core\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Workspace, Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "from azureml.train.dnn import TensorFlow\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "CT_NAME = \"ds3v2-clust\"           # Name of our compute cluster\n",
    "VM_SIZE = \"STANDARD_DS3_V2\"          # Specify the Azure VM for execution of our pipeline\n",
    "MIN_NODES = 1                    # Min number of compute nodes in cluster\n",
    "MAX_NODES = 4                    # Max number of compute nodes in cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reference to the workspace\n",
    "ws = Workspace.from_config(\"./ws_config.json\")\n",
    "\n",
    "# Set workspace's environment\n",
    "env = Environment.from_pip_requirements(name = \"HIFIS_env\", file_path = \"./../requirements.txt\")\n",
    "env.register(workspace=ws)\n",
    "runconfig = RunConfiguration(conda_dependencies=env.python.conda_dependencies)\n",
    "print(env.python.conda_dependencies.serialize_to_string())\n",
    "\n",
    "# Move AML ignore file to root folder\n",
    "aml_ignore_path = shutil.copy('./.amlignore', './../.amlignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the blob datastore associated with this workspace\n",
    "blob_store = Datastore(ws, name='hifis_v2_ds')\n",
    "\n",
    "# Create data references to folders on the blob\n",
    "data_dr = DataReference(\n",
    "    datastore=blob_store,\n",
    "    data_reference_name=\"data\",\n",
    "    path_on_datastore=\"data/\")\n",
    "training_logs_dr = DataReference(\n",
    "    datastore=blob_store,\n",
    "    data_reference_name=\"training_logs_data\",\n",
    "    path_on_datastore=\"logs/training/\")\n",
    "models_dr = DataReference(\n",
    "    datastore=blob_store,\n",
    "    data_reference_name=\"models_data\",\n",
    "    path_on_datastore=\"models/\")\n",
    "\n",
    "# Set up references to pipeline data (intermediate pipeline storage).\n",
    "processed_pd = PipelineData(\n",
    "    \"processed_data\",\n",
    "    datastore=blob_store,\n",
    "    output_name=\"processed_data\",\n",
    "    output_mode=\"mount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the compute target for this experiment\n",
    "try:\n",
    "    compute_target = AmlCompute(ws, CT_NAME)\n",
    "    print(\"Found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new compute target\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=VM_SIZE, min_nodes=1, max_nodes=4)    \n",
    "    compute_target = ComputeTarget.create(ws, CT_NAME, provisioning_config)  # Create the compute cluster\n",
    "    \n",
    "    # Wait for cluster to be provisioned\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20) \n",
    "    \n",
    "print(\"Azure Machine Learning Compute attached\")\n",
    "print(\"Compute targets: \", ws.compute_targets)\n",
    "compute_target = ws.compute_targets[CT_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing step the ML pipeline\n",
    "step1 = PythonScriptStep(name=\"preprocess_step\",\n",
    "                         script_name=\"azure/preprocess_step/preprocess_step.py\",\n",
    "                         arguments=[\"--datadir\", data_dr, \"--preprocesseddir\", processed_pd],\n",
    "                         inputs=[data_dr],\n",
    "                         outputs=[processed_pd],\n",
    "                         compute_target=compute_target, \n",
    "                         source_directory=\"./../\",\n",
    "                         runconfig=runconfig,\n",
    "                         allow_reuse=True)\n",
    "\n",
    "# Define training step in the ML pipeline\n",
    "est = TensorFlow(source_directory='./../',\n",
    "                   script_params=None,\n",
    "                   compute_target=compute_target,\n",
    "                   entry_script='azure/train_step/train_step.py',\n",
    "                   pip_packages=['tensorboard', 'pandas', 'dill', 'numpy', 'imblearn', 'matplotlib', 'tqdm', 'scikit-learn',\n",
    "                                'category_encoders'],\n",
    "                   use_gpu=False,\n",
    "                   framework_version='2.0')\n",
    "step2 = EstimatorStep(name=\"estimator_train_step\", \n",
    "                      estimator=est, \n",
    "                      estimator_entry_script_arguments=[\"--preprocesseddir\", processed_pd, \"--datadir\", data_dr,  \n",
    "                                                        \"--traininglogsdir\", training_logs_dr, \"--modelsdir\", models_dr],\n",
    "                      runconfig_pipeline_params=None, \n",
    "                      inputs=[processed_pd, data_dr, training_logs_dr, models_dr], \n",
    "                      outputs=[], \n",
    "                      compute_target=compute_target)\n",
    "\n",
    "# Construct the ML pipeline from the steps\n",
    "steps = [step1, step2]\n",
    "single_train_pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "single_train_pipeline.validate()\n",
    "\n",
    "# Define a new experiment and submit a new pipeline run to the compute target.\n",
    "experiment = Experiment(workspace=ws, name='MultiTrainExperiment_v1')\n",
    "train_run = experiment.submit(single_train_pipeline, regenerate_outputs=False)\n",
    "print(\"Pipeline is submitted for execution\")\n",
    "\n",
    "# Move AML ignore file back to original folder\n",
    "aml_ignore_path = shutil.move(aml_ignore_path, './.amlignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the pipeline to finish running.\n",
    "train_run.wait_for_completion()\n",
    "\n",
    "# Delete compute cluster to avoid extra charges\n",
    "compute_target.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
