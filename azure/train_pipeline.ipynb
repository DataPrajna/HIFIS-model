{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure ML Training Pipeline for HIFIS-v2\n",
    "This notebook defines an Azure machine learning pipeline for a multi-train experiment and submits the pipeline as an experiment to be run on an Azure virtual machine. It then publishes the pipeline in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import azureml.core\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Workspace, Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "from azureml.train.dnn import TensorFlow\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "import shutil\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the workspace and configure its Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Conda environment specification. The dependencies defined in this file will\r\n",
      "# be automatically provisioned for runs with userManagedDependencies=False.\r\n",
      "\n",
      "# Details about the Conda environment file format:\r\n",
      "# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually\r\n",
      "\n",
      "name: project_environment\n",
      "dependencies:\n",
      "  # The python interpreter version.\r\n",
      "  # Currently Azure ML only supports 3.5.2 and later.\r\n",
      "- python=3.6.2\n",
      "\n",
      "- pip:\n",
      "  - pyyaml==5.2\n",
      "  - matplotlib==3.1.1\n",
      "  - numpy==1.17.4\n",
      "  - tensorflow_gpu==2.0.1\n",
      "  - tensorboard==2.0.2\n",
      "  - lime==0.1.1.37\n",
      "  - scipy==1.4.1\n",
      "  - category_encoders==2.1.0\n",
      "  - imbalanced_learn==0.6.1\n",
      "  - tqdm==4.40.2\n",
      "  - pandas==0.25.3\n",
      "  - kmodes==0.10.2\n",
      "  - dill==0.3.0\n",
      "channels:\n",
      "- anaconda\n",
      "- conda-forge\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get reference to the workspace\n",
    "ws = Workspace.from_config(\"./ws_config.json\")\n",
    "\n",
    "# Set workspace's environment\n",
    "env = Environment.from_pip_requirements(name = \"HIFIS_env\", file_path = \"./../requirements.txt\")\n",
    "env.register(workspace=ws)\n",
    "runconfig = RunConfiguration(conda_dependencies=env.python.conda_dependencies)\n",
    "print(env.python.conda_dependencies.serialize_to_string())\n",
    "\n",
    "# Move AML ignore file to root folder\n",
    "aml_ignore_path = shutil.copy('./.amlignore', './../.amlignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create references to persistent and intermediate data\n",
    "Create DataReference objects that point to our raw data on the blob. Configure a PipelineData object to point to preprocessed data stored on the blob. Pipeline data is intermediate, meaning that it is produced by a step and will be fed as input to a subsequent step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the blob datastores associated with this workspace\n",
    "hifis_blob_ds = Datastore(ws, name='hifis_v2_ds')\n",
    "raw_data_blob_ds = Datastore(ws, name='hifis_raw_ds')\n",
    "\n",
    "# Create data references to folders on the blobs\n",
    "raw_data_dr = DataReference(\n",
    "    datastore=raw_data_blob_ds,\n",
    "    data_reference_name=\"raw_data\",\n",
    "    path_on_datastore=\"hifis/\")\n",
    "outputs_dr = DataReference(\n",
    "    datastore=hifis_blob_ds,\n",
    "    data_reference_name=\"outputs\",\n",
    "    path_on_datastore=\"outputs/\")\n",
    "\n",
    "# Set up references to pipeline data (intermediate pipeline storage).\n",
    "preprocess_pd = PipelineData(\n",
    "    \"preprocessed_output\",\n",
    "    datastore=hifis_blob_ds,\n",
    "    output_name=\"preprocessed_output\",\n",
    "    output_mode=\"mount\")\n",
    "train_pd = PipelineData(\n",
    "    \"train_output\",\n",
    "    datastore=hifis_blob_ds,\n",
    "    output_name=\"train_output\",\n",
    "    output_mode=\"mount\")\n",
    "interpretability_pd = PipelineData(\n",
    "    \"interpretability_output\",\n",
    "    datastore=hifis_blob_ds,\n",
    "    output_name=\"interpretability_output\",\n",
    "    output_mode=\"mount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Target\n",
    "Specify and configure the compute target for this workspace. If a compute cluster by the name we specified does not exist, create a new compute cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n",
      "Azure Machine Learning Compute attached\n",
      "Compute targets:  {'ds3v2-cluster': AmlCompute(workspace=Workspace.create(name='hifis-v2-ws-canada', subscription_id='4d8a1c27-3a23-4f81-8fec-5b598044652e', resource_group='hifis-v2-rg'), name=ds3v2-cluster, id=/subscriptions/4d8a1c27-3a23-4f81-8fec-5b598044652e/resourceGroups/hifis-v2-rg/providers/Microsoft.MachineLearningServices/workspaces/hifis-v2-ws-canada/computes/ds3v2-cluster, type=AmlCompute, provisioning_state=Succeeded, location=canadacentral, tags=None), 'ds3v2-train': AmlCompute(workspace=Workspace.create(name='hifis-v2-ws-canada', subscription_id='4d8a1c27-3a23-4f81-8fec-5b598044652e', resource_group='hifis-v2-rg'), name=ds3v2-train, id=/subscriptions/4d8a1c27-3a23-4f81-8fec-5b598044652e/resourceGroups/hifis-v2-rg/providers/Microsoft.MachineLearningServices/workspaces/hifis-v2-ws-canada/computes/ds3v2-train, type=AmlCompute, provisioning_state=Succeeded, location=canadacentral, tags=None)}\n"
     ]
    }
   ],
   "source": [
    "# Define some constants\n",
    "CT_NAME = \"ds3v2-train\"          # Name of our compute cluster\n",
    "VM_SIZE = \"STANDARD_DS3_V2\"      # Specify the Azure VM for execution of our pipeline\n",
    "MIN_NODES = 1                    # Min number of compute nodes in cluster\n",
    "MAX_NODES = 4                    # Max number of compute nodes in cluster\n",
    "\n",
    "# Set up the compute target for this experiment\n",
    "try:\n",
    "    compute_target = AmlCompute(ws, CT_NAME)\n",
    "    print(\"Found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new compute target\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=VM_SIZE, min_nodes=MIN_NODES, max_nodes=MAX_NODES)    \n",
    "    compute_target = ComputeTarget.create(ws, CT_NAME, provisioning_config)  # Create the compute cluster\n",
    "    \n",
    "    # Wait for cluster to be provisioned\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20) \n",
    "    \n",
    "print(\"Azure Machine Learning Compute attached\")\n",
    "print(\"Compute targets: \", ws.compute_targets)\n",
    "compute_target = ws.compute_targets[CT_NAME]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define pipeline and submit experiment.\n",
    "Define the steps of an Azure machine learning pipeline. Create an Azure Experiment that will run our pipeline. Submit the experiment to the execution environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step preprocess_step is ready to be created [111d98d4]\n",
      "Step multi_train_step is ready to be created [69b74aba]\n",
      "Step interpretability_step is ready to be created [80c627c2]\n",
      "Step save_step is ready to be created [8a735675]\n",
      "Created step preprocess_step [111d98d4][8e9118a0-a2a5-4501-a096-bd3e74d87777], (This step will run and generate new outputs)\n",
      "Created step multi_train_step [69b74aba][6574f9fc-7cb2-49bf-bff3-0109e188aea0], (This step will run and generate new outputs)\n",
      "Created step interpretability_step [80c627c2][f3d8200b-3563-4740-98ed-b7396d54325a], (This step will run and generate new outputs)\n",
      "Created step save_step [8a735675][698ebe33-90fb-42a9-ab13-9bd4d4ac7c79], (This step will run and generate new outputs)\n",
      "Using data reference raw_data for StepId [8daae77c][c88b15f3-6e00-4ddd-b91e-751c333763ff], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference outputs for StepId [74065a21][7f87b0ac-fd71-424a-a9e6-8d7dc3d6a830], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Submitted PipelineRun f49de01c-194e-40c7-bc89-4583a4bb8312\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/MultiTrainExperiment_v1/runs/f49de01c-194e-40c7-bc89-4583a4bb8312?wsid=/subscriptions/4d8a1c27-3a23-4f81-8fec-5b598044652e/resourcegroups/hifis-v2-rg/workspaces/hifis-v2-ws-canada\n",
      "Pipeline is submitted for execution\n"
     ]
    }
   ],
   "source": [
    "# Define preprocessing step in the ML pipeline\n",
    "step1 = PythonScriptStep(name=\"preprocess_step\",\n",
    "                         script_name=\"azure/preprocess_step/preprocess_step.py\",\n",
    "                         arguments=[\"--rawdatadir\", raw_data_dr, \"--preprocessedoutputdir\", preprocess_pd],\n",
    "                         inputs=[raw_data_dr],\n",
    "                         outputs=[preprocess_pd],\n",
    "                         compute_target=compute_target, \n",
    "                         source_directory=\"./../\",\n",
    "                         runconfig=runconfig,\n",
    "                         params={\"PIPELINE\": \"train\"},\n",
    "                         allow_reuse=False)\n",
    "\n",
    "# Define training step in the ML pipeline\n",
    "est = TensorFlow(source_directory='./../',\n",
    "                   script_params=None,\n",
    "                   compute_target=compute_target,\n",
    "                   entry_script='azure/train_step/train_step.py',\n",
    "                   pip_packages=['tensorboard', 'pandas', 'dill', 'numpy', 'imblearn', 'matplotlib', 'tqdm', 'scikit-learn',\n",
    "                                'category_encoders'],\n",
    "                   use_gpu=True,\n",
    "                   framework_version='2.0')\n",
    "step2 = EstimatorStep(name=\"multi_train_step\", \n",
    "                      estimator=est, \n",
    "                      estimator_entry_script_arguments=[\"--preprocessedoutputdir\", preprocess_pd, \"--trainoutputdir\", train_pd],\n",
    "                      runconfig_pipeline_params=None, \n",
    "                      inputs=[preprocess_pd], \n",
    "                      outputs=[train_pd], \n",
    "                      compute_target=compute_target)\n",
    "\n",
    "# Define interpretability step in the ML pipeline\n",
    "step3 = PythonScriptStep(name=\"interpretability_step\",\n",
    "                         script_name=\"azure/interpretability_step/interpretability_step.py\",\n",
    "                         arguments=[\"--preprocessedoutputdir\", preprocess_pd, \"--trainoutputdir\", train_pd, \n",
    "                                    \"--interpretabilityoutputdir\", interpretability_pd],\n",
    "                         inputs=[preprocess_pd, train_pd],\n",
    "                         outputs=[interpretability_pd],\n",
    "                         compute_target=compute_target, \n",
    "                         source_directory=\"./../\",\n",
    "                         runconfig=runconfig,\n",
    "                         allow_reuse=False)\n",
    "\n",
    "# Define final step to save all produced files to persistent blob storage\n",
    "step4 = PythonScriptStep(name=\"save_step\",\n",
    "                         script_name=\"azure/save_step/save_step.py\",\n",
    "                         arguments=[\"--preprocessedoutputdir\", preprocess_pd, \"--trainoutputdir\", train_pd, \n",
    "                                    \"--interpretabilityoutputdir\", interpretability_pd, \"--outputsdir\", outputs_dr],\n",
    "                         inputs=[preprocess_pd, train_pd, interpretability_pd, outputs_dr],\n",
    "                         outputs=[],\n",
    "                         compute_target=compute_target, \n",
    "                         source_directory=\"./../\",\n",
    "                         runconfig=runconfig,\n",
    "                         allow_reuse=False)\n",
    "\n",
    "# Construct the ML pipeline from the steps\n",
    "steps = [step1, step2, step3, step4]\n",
    "single_train_pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "single_train_pipeline.validate()\n",
    "\n",
    "# Define a new experiment and submit a new pipeline run to the compute target.\n",
    "experiment = Experiment(workspace=ws, name='MultiTrainExperiment_v1')\n",
    "train_run = experiment.submit(single_train_pipeline, regenerate_outputs=False)\n",
    "print(\"Pipeline is submitted for execution\")\n",
    "\n",
    "# Move AML ignore file back to original folder\n",
    "aml_ignore_path = shutil.move(aml_ignore_path, './.amlignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish the pipeline\n",
    "Wait for the pipeline run to finish. Then publish the pipeline. The pipeline will be visible as an endpoint in the Pipelines tab in the workspace on Azure Machine Learning studio. Delete the training compute cluster to prevent further cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the pipeline to finish running.\n",
    "train_run.wait_for_completion()\n",
    "\n",
    "# Publish the pipeline.\n",
    "published_pipeline = train_run.publish_pipeline(\n",
    "     name=\"HIFIS-v2 Training Pipeline\",\n",
    "     description=\"Azure ML Pipeline that trains HIFIS-v2 model and runs LIME submodular pick.\",\n",
    "     version=\"1.0\")\n",
    "\n",
    "# Delete compute cluster to avoid extra charges\n",
    "#compute_target.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
